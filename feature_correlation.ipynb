{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of data types, \"ordered\" data refers to ordinal data.\n",
    "Ordinal data is a type of categorical data with an order (or rank).\n",
    "The order of these values is significant and typically represents some sort of hierarchy.\n",
    "For example, ratings data (like \"poor\", \"average\", \"good\", \"excellent\") is ordinal\n",
    "because there is a clear order to the categories.\n",
    "\n",
    "\n",
    "1. **age** - Real\n",
    "2. **sex** - Binary\n",
    "3. **cp** - Chest pain type (4 values) - Nominal\n",
    "4. **trestbps** - Resting blood age - Real\n",
    "5. **chol** - Serum cholesterol (in mg/dl) - Real\n",
    "6. **fbs** - Fasting blood sugar > 120 mg/dl - Binary\n",
    "7. **restecg** - Resting electrocardiographic results (values 0,1,2) - Nominal\n",
    "8. **thalach** - Maximum heart rate achieved - Real\n",
    "9. **exang** - Exercise induced angina - Binary\n",
    "10. **oldpeak** - Oldpeak = ST depression induced by exercise relative to rest - Real\n",
    "11. **slope** - The slope of the peak exercise ST segment - Ordered\n",
    "12. **ca** - Number of major vessels (0-3) colored by flouroscopy - Real\n",
    "13. **thal** - Thal: 3 = normal; 6 = fixed defect; 7 = reversable defect - Nominal\n",
    "14. **target**: 1 = no disease; 2 = presence of disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "detail = {\"age\": \"Age\", \"sex\": \"Sex\", \"cp\": \"Chest Pain Type\", \"trestbps\": \"Resting Blood Pressure\",\n",
    "          \"chol\": \"Serum Cholesterol\", \"fbs\": \"Fasting Blood Sugar\", \"restecg\": \"Resting ECG\",\n",
    "          \"thalach\": \"Max Heart Rate\", \"exang\": \"Exercise Induced Angina\", \"oldpeak\": \"Oldpeak\",\n",
    "          \"slope\": \"Slope\", \"ca\": \"Number of major vessels\", \"thal\": \"Thal\", \"target\": \"(0 - no disease, 1 - disease))\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values: 0 \n",
      "\n",
      "Number of duplicates: 0 \n",
      "\n",
      "Number of features:  13 \n",
      "\n",
      "    age  trestbps   chol  thalach  oldpeak   ca\n",
      "0  70.0     130.0  322.0    109.0      2.4  3.0\n",
      "1  67.0     115.0  564.0    160.0      1.6  0.0\n",
      "2  57.0     124.0  261.0    141.0      0.3  0.0\n",
      "3  64.0     128.0  263.0    105.0      0.2  1.0\n",
      "4  74.0     120.0  269.0    121.0      0.2  1.0 \n",
      "\n",
      "        age  trestbps      chol   thalach   oldpeak        ca\n",
      "0  0.854167  0.339623  0.447489  0.290076  0.387097  1.000000\n",
      "1  0.791667  0.198113  1.000000  0.679389  0.258065  0.000000\n",
      "2  0.583333  0.283019  0.308219  0.534351  0.048387  0.000000\n",
      "3  0.729167  0.320755  0.312785  0.259542  0.032258  0.333333\n",
      "4  0.937500  0.245283  0.326484  0.381679  0.032258  0.333333 \n",
      "\n",
      "        age  sex  trestbps      chol  fbs   thalach  exang   oldpeak  \\\n",
      "0  0.854167  1.0  0.339623  0.447489  0.0  0.290076    0.0  0.387097   \n",
      "1  0.791667  0.0  0.198113  1.000000  0.0  0.679389    0.0  0.258065   \n",
      "2  0.583333  1.0  0.283019  0.308219  0.0  0.534351    0.0  0.048387   \n",
      "3  0.729167  1.0  0.320755  0.312785  0.0  0.259542    1.0  0.032258   \n",
      "4  0.937500  0.0  0.245283  0.326484  0.0  0.381679    1.0  0.032258   \n",
      "\n",
      "         ca  cp_1.0  ...  cp_4.0  restecg_0.0  restecg_1.0  restecg_2.0  \\\n",
      "0  1.000000       0  ...       1            0            0            1   \n",
      "1  0.000000       0  ...       0            0            0            1   \n",
      "2  0.000000       0  ...       0            1            0            0   \n",
      "3  0.333333       0  ...       1            1            0            0   \n",
      "4  0.333333       0  ...       0            0            0            1   \n",
      "\n",
      "   slope_1.0  slope_2.0  slope_3.0  thal_3.0  thal_6.0  thal_7.0  \n",
      "0          0          1          0         1         0         0  \n",
      "1          0          1          0         0         0         1  \n",
      "2          1          0          0         0         0         1  \n",
      "3          0          1          0         0         0         1  \n",
      "4          1          0          0         1         0         0  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "sns.set_theme(context=\"paper\", font_scale=1.5, style=\"whitegrid\", palette=\"Set2\")\n",
    "\n",
    "data = pd.read_csv(\"heart.dat\", sep=\"\\\\s+\", header=None)\n",
    "\n",
    "data.columns = detail.keys()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Number of missing values:\", data.isnull().sum().sum(), \"\\n\")\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"Number of duplicates:\", data.duplicated().sum(), \"\\n\")\n",
    "\n",
    "noFeatures = data.shape[1]-1\n",
    "print(\"Number of features: \", noFeatures, \"\\n\")\n",
    "\n",
    "continuousFeatures = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\", \"ca\"]\n",
    "\n",
    "X = data.iloc[:, :-1]\n",
    "Y = data.iloc[:, -1]\n",
    "\n",
    "print(X[continuousFeatures].head(), \"\\n\")\n",
    "# Apply scaler only to continuous variables\n",
    "X[continuousFeatures] = StandardScaler().fit_transform(X[continuousFeatures])\n",
    "#X[continuousFeatures] = MinMaxScaler().fit_transform(X[continuousFeatures])\n",
    "\n",
    "print(X[continuousFeatures].head(), \"\\n\")\n",
    "\n",
    "one_hot_X = pd.get_dummies(X, columns=[\"cp\", \"restecg\", \"slope\", \"thal\"])\n",
    "\n",
    "print(one_hot_X.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "realFeatures = (\"trestbps\", \"chol\", \"thalach\", \"oldpeak\")\n",
    "# considering ca ordered as only has 4 values\n",
    "categoricalFeatures = (\"cp\", \"fbs\", \"restecg\", \"exang\", \"slope\", \"thal\", \"ca\")\n",
    "extraFeatures = (\"age\", \"sex\", \"target\")\n",
    "\n",
    "# Age vs. All Real for both Sexes\n",
    "for feature in data.columns:\n",
    "    if feature in realFeatures:\n",
    "        plt.figure()\n",
    "        sns.relplot(    \n",
    "                data=data, x=\"age\", y=feature, col=\"sex\",\n",
    "                hue=\"target\"\n",
    "        )\n",
    "        plt.savefig(f\"plots/{feature}_vs_age.png\")\n",
    "\n",
    "# Categorical Counts\n",
    "for feature in data.columns:\n",
    "    if feature in categoricalFeatures:\n",
    "        plt.figure()\n",
    "        sns.countplot(  # histplot if for continuous non categorical data\n",
    "            data=data, x=feature, hue=\"target\"\n",
    "        )\n",
    "        plt.title(f\"{detail[feature]} {detail['target']}\")\n",
    "        plt.savefig(f\"plots/{feature}_count.png\")\n",
    "\n",
    "# Categorical Normalized Counts\n",
    "# for feature in data.columns:\n",
    "#     if feature in categoricalFeatures:\n",
    "#         # thanks copilot\n",
    "#         proportions = data.groupby(feature)[\"target\"].value_counts(normalize=True).rename(\"proportion\").reset_index()\n",
    "\n",
    "#         plt.figure()\n",
    "#         sns.barplot(data=proportions, x=feature, y=\"proportion\", hue=\"target\")\n",
    "#         plt.title(f\"{detail[feature]} {detail['target']}\")\n",
    "#         plt.show()\n",
    "\n",
    "# for feature in data.columns[:-1]:\n",
    "#     plotter2v2(data, feature, \"target\")\n",
    "\n",
    "# normalizer = StandardScaler()\n",
    "# calculate and remove mean and standard deviation from the data\n",
    "# data_scaled = pd.DataFrame(normalizer.fit_transform(data), columns=data.columns)\n",
    "\n",
    "\n",
    "\n",
    "# print(data.head())\n",
    "# sns.heatmap(data_scaled.corr(), annot=True, linewidths=2)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics Table:\n",
      "   Iteration  Accuracy  Precision    Recall  F1 Score\n",
      "0          1  0.740741   0.766667  0.766667  0.766667\n",
      "1          2  0.870370   0.875000  0.903226  0.888889\n",
      "2          3  0.796296   0.888889  0.750000  0.813559\n",
      "3          4  0.759259   0.800000  0.774194  0.786885\n",
      "4          5  0.740741   0.687500  0.846154  0.758621\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create empty lists to store the evaluation metrics\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "\n",
    "\n",
    "for random_state in range(5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=random_state)\n",
    "    # Perform further analysis or model training with the current split\n",
    "    # Create a Naive Bayes classifier\n",
    "    model = GaussianNB()\n",
    "\n",
    "    # Train the model using the training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    # pos_label referes to the HD presence class (the positive class)\n",
    "    pos_label = 1\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, pos_label=pos_label)\n",
    "    recall = recall_score(y_test, y_pred, pos_label=pos_label)\n",
    "    f1 = f1_score(y_test, y_pred, pos_label=pos_label)\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Append the evaluation metrics to the respective lists\n",
    "    accuracy_list.append(accuracy)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(f1)\n",
    "\n",
    "    # Print the evaluation metrics for the current iteration\n",
    "    # print(\"Confusion Matrix:\")\n",
    "    # print(pd.DataFrame(cm, columns=[\"Predicted HD Absence\", \"Predicted HD Presence\"], index=[\"Actual HD Absence\", \"Actual HD Presence\"]))\n",
    "    # print()\n",
    "\n",
    "\n",
    "# Create a DataFrame to store the evaluation metrics for each iteration\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Iteration\": range(1, 6),\n",
    "    \"Accuracy\": accuracy_list,\n",
    "    \"Precision\": precision_list,\n",
    "    \"Recall\": recall_list,\n",
    "    \"F1 Score\": f1_list\n",
    "})\n",
    "\n",
    "# Print the metrics table\n",
    "print(\"Metrics Table:\")\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Training Data:  0.8518518518518519\n",
      "Accuracy on Test Data:  0.7777777777777778\n",
      "Accuracy on Training Data:  0.8518518518518519\n",
      "Accuracy on Test Data:  0.7777777777777778\n",
      "Accuracy on Training Data:  0.8518518518518519\n",
      "Accuracy on Test Data:  0.7777777777777778\n",
      "Accuracy on Training Data:  0.8518518518518519\n",
      "Accuracy on Test Data:  0.7777777777777778\n",
      "Accuracy on Training Data:  0.8518518518518519\n",
      "Accuracy on Test Data:  0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create empty lists to store the evaluation metrics\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "\n",
    "\n",
    "for random_state in range(5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(one_hot_X, Y, test_size=0.2, random_state=random_state)\n",
    "    # Dividir os dados em training data & Test data, de forma aleatória\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=2)\n",
    "\n",
    "    #Logistic Regression\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    #Treinar o modelo Logistic Regression com a Training Data\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    # Avaliação do Modelo\n",
    "\n",
    "    #Accuracy on training data\n",
    "    X_train_prediction=model.predict(X_train)\n",
    "    training_data_accuracy = accuracy_score(X_train_prediction, Y_train)\n",
    "    print('Accuracy on Training Data: ', training_data_accuracy)\n",
    "\n",
    "    #Accuracy on test data\n",
    "    X_test_prediction=model.predict(X_test)\n",
    "    test_data_accuracy = accuracy_score(X_test_prediction, Y_test)\n",
    "    print('Accuracy on Test Data: ', test_data_accuracy)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    # pos_label referes to the HD presence class (the positive class)\n",
    "    pos_label = 1\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, pos_label=pos_label)\n",
    "    recall = recall_score(y_test, y_pred, pos_label=pos_label)\n",
    "    f1 = f1_score(y_test, y_pred, pos_label=pos_label)\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Append the evaluation metrics to the respective lists\n",
    "    accuracy_list.append(accuracy)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(f1)\n",
    "\n",
    "\n",
    "# Sistema\n",
    "\n",
    "# input_data = (52,1,0,125,212,0,1,168,0,1,2,2,3) # Estes valores são os da primeira linha do heart.csv\n",
    "\n",
    "# input_data_as_numpy_array = np.asarray(input_data) #Converter a input data num array\n",
    "# input_data_reshaped = input_data_as_numpy_array.reshape(1,-1) #Reshape do array em uma linha em quantas colunas necessária para preservar o número original de elementos\n",
    "\n",
    "# prediction = model.predict(input_data_reshaped)\n",
    "# print(prediction)\n",
    "\n",
    "# if(prediction[0]==0):\n",
    "#     print('The patient does not have a Heart Disease')\n",
    "# else:\n",
    "#     print('The patient has a Heart Disease')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
