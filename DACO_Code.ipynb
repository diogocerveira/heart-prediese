{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of data types, \"ordered\" data refers to ordinal data.\n",
    "Ordinal data is a type of categorical data with an order (or rank).\n",
    "The order of these values is significant and typically represents some sort of hierarchy.\n",
    "For example, ratings data (like \"poor\", \"average\", \"good\", \"excellent\") is ordinal\n",
    "because there is a clear order to the categories.\n",
    "\n",
    "| Feature         | Description                                                  | Type\n",
    "| ---             | ---                                                          | ---                                    \n",
    "| **age**         | Age                                                          | Real\n",
    "| **sex**         | Sex                                                          | Binary\n",
    "| **cp**          | Chest pain type (4 values)                                   | Nominal\n",
    "| **trestbps**    | Resting blood age                                            | Real\n",
    "| **chol**        | Serum cholesterol (in mg/dl)                                 | Real\n",
    "| **fbs**         | Fasting blood sugar > 120 mg/dl                              | Binary\n",
    "| **restecg**     | Resting electrocardiographic results (values 0,1,2)          | Nominal\n",
    "| **thalach**     | Maximum heart rate achieved                                  | Real\n",
    "| **exang**       | Exercise induced angina                                      | Binary\n",
    "| **oldpeak**     | Oldpeak = ST depression induced by exercise relative to rest | Real\n",
    "| **slope**       | The slope of the peak exercise ST segment                    | Ordered\n",
    "| **ca**          | Number of major vessels (0-3) colored by flouroscopy         | Real\n",
    "| **thal**        | Thal: 3 = normal; 6 = fixed defect; 7 = reversable defect    | Nominal\n",
    "| **target**      | 1 = no disease; 2 = presence of disease                      | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "detail = {\"age\": \"Age\", \"sex\": \"Sex\", \"cp\": \"Chest Pain Type\", \"trestbps\": \"Resting Blood Pressure\",\n",
    "          \"chol\": \"Serum Cholesterol\", \"fbs\": \"Fasting Blood Sugar\", \"restecg\": \"Resting ECG\",\n",
    "          \"thalach\": \"Max Heart Rate\", \"exang\": \"Exercise Induced Angina\", \"oldpeak\": \"Oldpeak\",\n",
    "          \"slope\": \"Slope\", \"ca\": \"Number of major vessels\", \"thal\": \"Thal\", \"target\": \"(0 - no disease, 1 - disease))\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "sns.set_theme(context=\"paper\", font_scale=1.5, style=\"whitegrid\", palette=\"Set2\")\n",
    "\n",
    "data = pd.read_csv(\"heart.dat\", sep=\"\\\\s+\", header=None)\n",
    "\n",
    "data.columns = detail.keys()\n",
    "print(data.head(), \"\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Number of missing values:\", data.isnull().sum().sum(), \"\\n\")\n",
    "# Check for duplicates\n",
    "print(\"Number of duplicates:\", data.duplicated().sum(), \"\\n\")\n",
    "\n",
    "continuousFeatures = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\", \"ca\"]\n",
    "\n",
    "X = data.iloc[:, :-1]\n",
    "Y = data.iloc[:, -1]\n",
    "\n",
    "# print(X[continuousFeatures].head(), \"\\n\")\n",
    "# Apply scaler only to continuous variables\n",
    "standardizedX = X.copy()\n",
    "standardizedX[continuousFeatures] = StandardScaler().fit_transform(X[continuousFeatures])\n",
    "\n",
    "normalizedX = X.copy()\n",
    "normalizedX[continuousFeatures] = MinMaxScaler().fit_transform(X[continuousFeatures])\n",
    "\n",
    "print(standardizedX[continuousFeatures].head(), \"\\n\")\n",
    "\n",
    "one_hot_X = pd.get_dummies(X, columns=[\"cp\", \"restecg\", \"slope\", \"thal\"])\n",
    "one_hot_standardizedX = pd.get_dummies(standardizedX, columns=[\"cp\", \"restecg\", \"slope\", \"thal\"])\n",
    "one_hot_normalizedX = pd.get_dummies(normalizedX, columns=[\"cp\", \"restecg\", \"slope\", \"thal\"])\n",
    "\n",
    "print(\"one hot\", one_hot_X.head(), \"\\n\")\n",
    "print(\"one hot standard\", one_hot_standardizedX.head(), \"\\n\")\n",
    "print(\"one hot norm\", one_hot_normalizedX.head(), \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Correlation\n",
    "\n",
    "## Heatmap\n",
    "\n",
    "Using only numeric features: **continuous** - age, trestbps, chol, thalach, oldpeak / **ordered** - ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# considering ca ordered as only has 4 values\n",
    "numericFeatures = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\", \"ca\"]\n",
    "categoricalFeatures = [\"sex\", \"cp\", \"fbs\", \"restecg\", \"exang\", \"slope\", \"thal\"]\n",
    "\n",
    "X_numeric = X_oneHot[numericFeatures]\n",
    "dataCorr = pd.concat([X_numeric, data[\"target\"]], axis=1).corr()\n",
    "\n",
    "upperHalf_mask = np.tril(np.ones_like(dataCorr, dtype=bool))\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Numeric Feature Heatmap\")\n",
    "sns.heatmap(dataCorr, annot=True, linewidths=2, mask=upperHalf_mask)\n",
    "plt.savefig(f\"plots/numeric_heatmap.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can choose to only include the 3 features that mostly correlate with the target: **thalach**, **oldpeak** and **ca**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_oneHot_heatmapped = data.drop([\"age\", \"trestbps\", \"chol\"], axis=1)\n",
    "\n",
    "# print(X_oneHot_heatmapped.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "[PCA Explanation](https://builtin.com/data-science/step-step-explanation-principal-component-analysis)  \n",
    "\n",
    "GitHub Copilot: Advantages of using PCA for dimensionality reduction:\n",
    "\n",
    "1. **Removes Correlated Features**: In the real world, features are often correlated. PCA allows you to identify the most important features of your dataset, reducing it to a smaller set of uncorrelated features, known as principal components.\n",
    "\n",
    "2. **Improves Algorithm Performance**: With fewer features, the performance of a machine learning algorithm can improve. It can also reduce overfitting.\n",
    "\n",
    "3. **Reduces Overfitting**: By reducing the dimensionality of your feature space, you're less likely to overfit your model.\n",
    "\n",
    "4. **Improves Visualization**: It's hard to visualize high dimensional data. PCA transforms a high dimensional data set to 2 or 3 dimensions so we can plot and understand data better.\n",
    "\n",
    "Disadvantages of using PCA:\n",
    "\n",
    "1. **Independent variables become less interpretable**: After implementing PCA, your original features will turn into Principal Components. Principal Components are the linear combination of your original features. Principal Components are not as readable and interpretable as original features.\n",
    "\n",
    "2. **Data standardization is must before PCA**: You must standardize your data before implementing PCA, otherwise PCA will not be able to find the optimal Principal Components.\n",
    "\n",
    "3. **Information Loss**: Although principal components attempt to retain as much information as possible, some information is lost when reducing dimensions, which can potentially degrade the performance of your machine learning model.\n",
    "\n",
    "4. **Doesn't handle non-linear features well**: PCA assumes that the principal components are a linear combination of the original features. If this assumption is not true, PCA may not give you the results you're looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=len(X_numeric.columns))\n",
    "pca_result = pca.fit_transform(X_numeric)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# plot no of components vs cumulative explained variance\n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(explained_variance, )\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the *Number of Components* against the *Cumulative Explained Variation*, we can see that 5 principal components are useful to explain 100% of the variance, the same number of numeric features. PCA won't provide the benefit of reducing dimensionality of the dataset.\n",
    "\n",
    "Copilot:\n",
    "\n",
    "That said, PCA can still be useful in this case for other reasons:\n",
    "\n",
    "1. **Feature Independence**: The PCs are linearly independent of each other, which can help with certain types of models that assume feature independence (like linear regression).\n",
    "\n",
    "2. **Interpretability**: PCs can sometimes be interpreted in terms of the original features, which can provide insights into the structure of your data.\n",
    "\n",
    "3. **Noise Reduction**: PCA can help to reduce noise in your data by focusing on the directions of maximum variance and ignoring smaller, potentially noisy fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "nPCs = 5\n",
    "pca = PCA(n_components=nPCs)\n",
    "pca_result = pca.fit_transform(X)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Explained Variance: {explained_variance}\")\n",
    "# print(pca.components_)  # feature weight for each pc\n",
    "\n",
    "# Convert it back to a DataFrame\n",
    "pca_df = pd.DataFrame(data=pca_result, columns=[\"PC\" + str(i + 1) for i in range(nPCs)])\n",
    "\n",
    "\n",
    "X_oneHot_PCAed = pd.concat([pca_df, X_oneHot.drop(X_numeric.columns, axis=1)], axis=1)\n",
    "\n",
    "# print(X_oneHot_PCAed.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "continuousX=X[continuousFeatures].copy()\n",
    "print(continuousX.head())\n",
    "\n",
    "croppedX = continuousX.copy()\n",
    "croppedX = croppedX.drop([\"thalach\"], axis=1)\n",
    "print(croppedX.head())\n",
    "\n",
    "\n",
    "\n",
    "nb_results = cross_validate(GaussianNB(), X, Y, cv=5, scoring=[\"accuracy\",\"precision\", \"recall\", \"f1\", \"roc_auc\"])\n",
    "results_df = pd.DataFrame(nb_results)\n",
    "print(results_df.mean())\n",
    "\n",
    "nb_results = cross_validate(GaussianNB(), continuousX, Y, cv=5, scoring=[\"accuracy\",\"precision\", \"recall\", \"f1\", \"roc_auc\"])\n",
    "results_df = pd.DataFrame(nb_results)\n",
    "print(results_df.mean())\n",
    "\n",
    "nb_results = cross_validate(GaussianNB(), croppedX, Y, cv=5, scoring=[\"accuracy\",\"precision\", \"recall\", \"f1\", \"roc_auc\"])\n",
    "results_df = pd.DataFrame(nb_results)\n",
    "print(results_df.mean())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score_time        0.007607\n",
    "test_accuracy     0.744444\n",
    "test_precision    0.736559\n",
    "test_recall       0.846667\n",
    "test_f1           0.784980\n",
    "test_roc_auc      0.805278"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "\n",
    "\n",
    "# nb_results = cross_validate(LogisticRegression(), standardizedX, Y, cv=5, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"neg_log_loss\"])\n",
    "# results_df = pd.DataFrame(nb_results)\n",
    "# print(\"standardizedX\", \"\\n\", results_df.mean(), \"\\n\\n\")\n",
    "\n",
    "\n",
    "# nb_results = cross_validate(LogisticRegression(), normalizedX, Y, cv=5, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"neg_log_loss\"])\n",
    "# results_df = pd.DataFrame(nb_results)\n",
    "# print(\"normalizedX\",\"\\n\", results_df.mean(), \"\\n\\n\")\n",
    "\n",
    "nb_results = cross_validate(LogisticRegression(), one_hot_standardizedX, Y, cv=5, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"])\n",
    "results_df = pd.DataFrame(nb_results)\n",
    "print(\"one_hot_standardizedX\",\"\\n\", results_df.mean(), \"\\n\\n\")\n",
    "\n",
    "# nb_results = cross_validate(LogisticRegression(), one_hot_normalizedX, Y, cv=5, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"neg_log_loss\"])\n",
    "# results_df = pd.DataFrame(nb_results)\n",
    "# print(\"one_hot_normalizedX\",\"\\n\", results_df.mean(), \"\\n\\n\")\n",
    "\n",
    "param_grid = [    \n",
    "    {\n",
    "    'C' : [0.01,0.1,1,10,100],\n",
    "    }\n",
    "]\n",
    "\n",
    "clf = GridSearchCV(LogisticRegression(), param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"], return_train_score=False, refit=False)\n",
    "clf.fit(one_hot_standardizedX, Y)\n",
    "results_gs = pd.DataFrame(clf.cv_results_)\n",
    "results_gs[['param_C','mean_test_accuracy','mean_test_precision','mean_test_recall','mean_test_f1','mean_test_roc_auc']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = GridSearchCV(KNeighborsClassifier(), {\"n_neighbors\": [1, 3, 5, 7, 9]}, cv=5, scoring=[\"f1\", \"neg_log_loss\"], return_train_score=False, refit=False)\n",
    "clf.fit(one_hot_standardizedX, Y)\n",
    "results_gs = pd.DataFrame(clf.cv_results_)\n",
    "results_gs[['param_n_neighbors', 'mean_test_f1', 'mean_test_neg_log_loss']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
