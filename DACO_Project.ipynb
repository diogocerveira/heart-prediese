{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of data types, \"ordered\" data refers to ordinal data.\n",
    "Ordinal data is a type of categorical data with an order (or rank).\n",
    "The order of these values is significant and typically represents some sort of hierarchy.\n",
    "For example, ratings data (like \"poor\", \"average\", \"good\", \"excellent\") is ordinal\n",
    "because there is a clear order to the categories.\n",
    "\n",
    "| Feature         | Description                                                  | Type\n",
    "| ---             | ---                                                          | ---                                    \n",
    "| **age**         | Age                                                          | Real\n",
    "| **sex**         | Sex                                                          | Binary\n",
    "| **cp**          | Chest pain type (4 values)                                   | Nominal\n",
    "| **trestbps**    | Resting blood age                                            | Real\n",
    "| **chol**        | Serum cholesterol (in mg/dl)                                 | Real\n",
    "| **fbs**         | Fasting blood sugar > 120 mg/dl                              | Binary\n",
    "| **restecg**     | Resting electrocardiographic results (values 0,1,2)          | Nominal\n",
    "| **thalach**     | Maximum heart rate achieved                                  | Real\n",
    "| **exang**       | Exercise induced angina                                      | Binary\n",
    "| **oldpeak**     | Oldpeak = ST depression induced by exercise relative to rest | Real\n",
    "| **slope**       | The slope of the peak exercise ST segment                    | Ordered\n",
    "| **ca**          | Number of major vessels (0-3) colored by flouroscopy         | Real\n",
    "| **thal**        | Thal: 3 = normal; 6 = fixed defect; 7 = reversable defect    | Nominal\n",
    "| **target**      | 1 = no disease; 2 = presence of disease                      | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "detail = {\"age\": \"Age\", \"sex\": \"Sex\", \"cp\": \"Chest Pain Type\", \"trestbps\": \"Resting Blood Pressure\",\n",
    "          \"chol\": \"Serum Cholesterol\", \"fbs\": \"Fasting Blood Sugar\", \"restecg\": \"Resting ECG\",\n",
    "          \"thalach\": \"Max Heart Rate\", \"exang\": \"Exercise Induced Angina\", \"oldpeak\": \"Oldpeak\",\n",
    "          \"slope\": \"Slope\", \"ca\": \"Number of major vessels\", \"thal\": \"Thal\", \"target\": \"(0 - no disease, 1 - disease))\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
      "0  70.0  1.0  4.0     130.0  322.0  0.0      2.0    109.0    0.0      2.4   \n",
      "1  67.0  0.0  3.0     115.0  564.0  0.0      2.0    160.0    0.0      1.6   \n",
      "2  57.0  1.0  2.0     124.0  261.0  0.0      0.0    141.0    0.0      0.3   \n",
      "3  64.0  1.0  4.0     128.0  263.0  0.0      0.0    105.0    1.0      0.2   \n",
      "4  74.0  0.0  2.0     120.0  269.0  0.0      2.0    121.0    1.0      0.2   \n",
      "\n",
      "   slope   ca  thal  target  \n",
      "0    2.0  3.0   3.0       2  \n",
      "1    2.0  0.0   7.0       1  \n",
      "2    1.0  0.0   7.0       2  \n",
      "3    2.0  1.0   7.0       1  \n",
      "4    1.0  1.0   3.0       1   \n",
      "\n",
      "Number of missing values: 0 \n",
      "\n",
      "Number of duplicates: 0 \n",
      "\n",
      "        age  trestbps      chol   thalach   oldpeak        ca\n",
      "0  1.712094 -0.075410  1.402212 -1.759208  1.181012  2.472682\n",
      "1  1.382140 -0.916759  6.093004  0.446409  0.481153 -0.711535\n",
      "2  0.282294 -0.411950  0.219823 -0.375291 -0.656118 -0.711535\n",
      "3  1.052186 -0.187590  0.258589 -1.932198 -0.743600  0.349871\n",
      "4  2.152032 -0.636310  0.374890 -1.240239 -0.743600  0.349871 \n",
      "\n",
      "one hot     age  sex  trestbps   chol  fbs  thalach  exang  oldpeak   ca  cp_1.0  ...  \\\n",
      "0  70.0  1.0     130.0  322.0  0.0    109.0    0.0      2.4  3.0   False  ...   \n",
      "1  67.0  0.0     115.0  564.0  0.0    160.0    0.0      1.6  0.0   False  ...   \n",
      "2  57.0  1.0     124.0  261.0  0.0    141.0    0.0      0.3  0.0   False  ...   \n",
      "3  64.0  1.0     128.0  263.0  0.0    105.0    1.0      0.2  1.0   False  ...   \n",
      "4  74.0  0.0     120.0  269.0  0.0    121.0    1.0      0.2  1.0   False  ...   \n",
      "\n",
      "   cp_4.0  restecg_0.0  restecg_1.0  restecg_2.0  slope_1.0  slope_2.0  \\\n",
      "0    True        False        False         True      False       True   \n",
      "1   False        False        False         True      False       True   \n",
      "2   False         True        False        False       True      False   \n",
      "3    True         True        False        False      False       True   \n",
      "4   False        False        False         True       True      False   \n",
      "\n",
      "   slope_3.0  thal_3.0  thal_6.0  thal_7.0  \n",
      "0      False      True     False     False  \n",
      "1      False     False     False      True  \n",
      "2      False     False     False      True  \n",
      "3      False     False     False      True  \n",
      "4      False      True     False     False  \n",
      "\n",
      "[5 rows x 22 columns] \n",
      "\n",
      "one hot standard         age  sex  trestbps      chol  fbs   thalach  exang   oldpeak  \\\n",
      "0  1.712094  1.0 -0.075410  1.402212  0.0 -1.759208    0.0  1.181012   \n",
      "1  1.382140  0.0 -0.916759  6.093004  0.0  0.446409    0.0  0.481153   \n",
      "2  0.282294  1.0 -0.411950  0.219823  0.0 -0.375291    0.0 -0.656118   \n",
      "3  1.052186  1.0 -0.187590  0.258589  0.0 -1.932198    1.0 -0.743600   \n",
      "4  2.152032  0.0 -0.636310  0.374890  0.0 -1.240239    1.0 -0.743600   \n",
      "\n",
      "         ca  cp_1.0  ...  cp_4.0  restecg_0.0  restecg_1.0  restecg_2.0  \\\n",
      "0  2.472682   False  ...    True        False        False         True   \n",
      "1 -0.711535   False  ...   False        False        False         True   \n",
      "2 -0.711535   False  ...   False         True        False        False   \n",
      "3  0.349871   False  ...    True         True        False        False   \n",
      "4  0.349871   False  ...   False        False        False         True   \n",
      "\n",
      "   slope_1.0  slope_2.0  slope_3.0  thal_3.0  thal_6.0  thal_7.0  \n",
      "0      False       True      False      True     False     False  \n",
      "1      False       True      False     False     False      True  \n",
      "2       True      False      False     False     False      True  \n",
      "3      False       True      False     False     False      True  \n",
      "4       True      False      False      True     False     False  \n",
      "\n",
      "[5 rows x 22 columns] \n",
      "\n",
      "one hot norm         age  sex  trestbps      chol  fbs   thalach  exang   oldpeak  \\\n",
      "0  0.854167  1.0  0.339623  0.447489  0.0  0.290076    0.0  0.387097   \n",
      "1  0.791667  0.0  0.198113  1.000000  0.0  0.679389    0.0  0.258065   \n",
      "2  0.583333  1.0  0.283019  0.308219  0.0  0.534351    0.0  0.048387   \n",
      "3  0.729167  1.0  0.320755  0.312785  0.0  0.259542    1.0  0.032258   \n",
      "4  0.937500  0.0  0.245283  0.326484  0.0  0.381679    1.0  0.032258   \n",
      "\n",
      "         ca  cp_1.0  ...  cp_4.0  restecg_0.0  restecg_1.0  restecg_2.0  \\\n",
      "0  1.000000   False  ...    True        False        False         True   \n",
      "1  0.000000   False  ...   False        False        False         True   \n",
      "2  0.000000   False  ...   False         True        False        False   \n",
      "3  0.333333   False  ...    True         True        False        False   \n",
      "4  0.333333   False  ...   False        False        False         True   \n",
      "\n",
      "   slope_1.0  slope_2.0  slope_3.0  thal_3.0  thal_6.0  thal_7.0  \n",
      "0      False       True      False      True     False     False  \n",
      "1      False       True      False     False     False      True  \n",
      "2       True      False      False     False     False      True  \n",
      "3      False       True      False     False     False      True  \n",
      "4       True      False      False      True     False     False  \n",
      "\n",
      "[5 rows x 22 columns] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "sns.set_theme(context=\"paper\", font_scale=1.5, style=\"whitegrid\", palette=\"Set2\")\n",
    "\n",
    "data = pd.read_csv(\"heart.dat\", sep=\"\\\\s+\", header=None)\n",
    "\n",
    "data.columns = detail.keys()\n",
    "print(data.head(), \"\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Number of missing values:\", data.isnull().sum().sum(), \"\\n\")\n",
    "# Check for duplicates\n",
    "print(\"Number of duplicates:\", data.duplicated().sum(), \"\\n\")\n",
    "\n",
    "continuousFeatures = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\", \"ca\"]\n",
    "\n",
    "X = data.iloc[:, :-1]\n",
    "Y = data.iloc[:, -1]\n",
    "\n",
    "# print(X[continuousFeatures].head(), \"\\n\")\n",
    "# Apply scaler only to continuous variables\n",
    "standardizedX = X.copy()\n",
    "standardizedX[continuousFeatures] = StandardScaler().fit_transform(X[continuousFeatures])\n",
    "\n",
    "normalizedX = X.copy()\n",
    "normalizedX[continuousFeatures] = MinMaxScaler().fit_transform(X[continuousFeatures])\n",
    "\n",
    "print(standardizedX[continuousFeatures].head(), \"\\n\")\n",
    "\n",
    "one_hot_X = pd.get_dummies(X, columns=[\"cp\", \"restecg\", \"slope\", \"thal\"])\n",
    "one_hot_standardizedX = pd.get_dummies(standardizedX, columns=[\"cp\", \"restecg\", \"slope\", \"thal\"])\n",
    "one_hot_normalizedX = pd.get_dummies(normalizedX, columns=[\"cp\", \"restecg\", \"slope\", \"thal\"])\n",
    "\n",
    "print(\"one hot\", one_hot_X.head(), \"\\n\")\n",
    "print(\"one hot standard\", one_hot_standardizedX.head(), \"\\n\")\n",
    "print(\"one hot norm\", one_hot_normalizedX.head(), \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Correlation\n",
    "\n",
    "## Heatmap\n",
    "\n",
    "Using only numeric features: **continuous** - age, trestbps, chol, thalach, oldpeak / **ordered** - ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# considering ca ordered as only has 4 values\n",
    "numericFeatures = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\", \"ca\"]\n",
    "categoricalFeatures = [\"sex\", \"cp\", \"fbs\", \"restecg\", \"exang\", \"slope\", \"thal\"]\n",
    "\n",
    "X_numeric = X_oneHot[numericFeatures]\n",
    "dataCorr = pd.concat([X_numeric, data[\"target\"]], axis=1).corr()\n",
    "\n",
    "upperHalf_mask = np.tril(np.ones_like(dataCorr, dtype=bool))\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Numeric Feature Heatmap\")\n",
    "sns.heatmap(dataCorr, annot=True, linewidths=2, mask=upperHalf_mask)\n",
    "plt.savefig(f\"plots/numeric_heatmap.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can choose to only include the 3 features that mostly correlate with the target: **thalach**, **oldpeak** and **ca**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_oneHot_heatmapped = data.drop([\"age\", \"trestbps\", \"chol\"], axis=1)\n",
    "\n",
    "# print(X_oneHot_heatmapped.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "[PCA Explanation](https://builtin.com/data-science/step-step-explanation-principal-component-analysis)  \n",
    "\n",
    "GitHub Copilot: Advantages of using PCA for dimensionality reduction:\n",
    "\n",
    "1. **Removes Correlated Features**: In the real world, features are often correlated. PCA allows you to identify the most important features of your dataset, reducing it to a smaller set of uncorrelated features, known as principal components.\n",
    "\n",
    "2. **Improves Algorithm Performance**: With fewer features, the performance of a machine learning algorithm can improve. It can also reduce overfitting.\n",
    "\n",
    "3. **Reduces Overfitting**: By reducing the dimensionality of your feature space, you're less likely to overfit your model.\n",
    "\n",
    "4. **Improves Visualization**: It's hard to visualize high dimensional data. PCA transforms a high dimensional data set to 2 or 3 dimensions so we can plot and understand data better.\n",
    "\n",
    "Disadvantages of using PCA:\n",
    "\n",
    "1. **Independent variables become less interpretable**: After implementing PCA, your original features will turn into Principal Components. Principal Components are the linear combination of your original features. Principal Components are not as readable and interpretable as original features.\n",
    "\n",
    "2. **Data standardization is must before PCA**: You must standardize your data before implementing PCA, otherwise PCA will not be able to find the optimal Principal Components.\n",
    "\n",
    "3. **Information Loss**: Although principal components attempt to retain as much information as possible, some information is lost when reducing dimensions, which can potentially degrade the performance of your machine learning model.\n",
    "\n",
    "4. **Doesn't handle non-linear features well**: PCA assumes that the principal components are a linear combination of the original features. If this assumption is not true, PCA may not give you the results you're looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=len(X_numeric.columns))\n",
    "pca_result = pca.fit_transform(X_numeric)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# plot no of components vs cumulative explained variance\n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(explained_variance, )\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the *Number of Components* against the *Cumulative Explained Variation*, we can see that 5 principal components are useful to explain 100% of the variance, the same number of numeric features. PCA won't provide the benefit of reducing dimensionality of the dataset.\n",
    "\n",
    "Copilot:\n",
    "\n",
    "That said, PCA can still be useful in this case for other reasons:\n",
    "\n",
    "1. **Feature Independence**: The PCs are linearly independent of each other, which can help with certain types of models that assume feature independence (like linear regression).\n",
    "\n",
    "2. **Interpretability**: PCs can sometimes be interpreted in terms of the original features, which can provide insights into the structure of your data.\n",
    "\n",
    "3. **Noise Reduction**: PCA can help to reduce noise in your data by focusing on the directions of maximum variance and ignoring smaller, potentially noisy fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "nPCs = 5\n",
    "pca = PCA(n_components=nPCs)\n",
    "pca_result = pca.fit_transform(X)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Explained Variance: {explained_variance}\")\n",
    "# print(pca.components_)  # feature weight for each pc\n",
    "\n",
    "# Convert it back to a DataFrame\n",
    "pca_df = pd.DataFrame(data=pca_result, columns=[\"PC\" + str(i + 1) for i in range(nPCs)])\n",
    "\n",
    "\n",
    "X_oneHot_PCAed = pd.concat([pca_df, X_oneHot.drop(X_numeric.columns, axis=1)], axis=1)\n",
    "\n",
    "# print(X_oneHot_PCAed.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL FEATURES \n",
      " fit_time           0.004009\n",
      "score_time         0.016963\n",
      "test_accuracy      0.840741\n",
      "train_accuracy     0.862963\n",
      "test_precision     0.844892\n",
      "train_precision    0.869341\n",
      "test_recall        0.873333\n",
      "train_recall       0.886667\n",
      "test_f1            0.858787\n",
      "train_f1           0.877886\n",
      "test_roc_auc       0.900000\n",
      "train_roc_auc      0.918003\n",
      "dtype: float64 \n",
      "\n",
      "\n",
      "CONTINUOUS FEATURES \n",
      " fit_time           0.003253\n",
      "score_time         0.011207\n",
      "test_accuracy      0.762963\n",
      "train_accuracy     0.778704\n",
      "test_precision     0.775966\n",
      "train_precision    0.782461\n",
      "test_recall        0.806667\n",
      "train_recall       0.833333\n",
      "test_f1            0.788938\n",
      "train_f1           0.807065\n",
      "test_roc_auc       0.826667\n",
      "train_roc_auc      0.844167\n",
      "dtype: float64 \n",
      "\n",
      "\n",
      "CONTINUOUS FEATURES W/o THALACH \n",
      " fit_time           0.003309\n",
      "score_time         0.010276\n",
      "test_accuracy      0.744444\n",
      "train_accuracy     0.756481\n",
      "test_precision     0.736559\n",
      "train_precision    0.743835\n",
      "test_recall        0.846667\n",
      "train_recall       0.856667\n",
      "test_f1            0.784980\n",
      "train_f1           0.796089\n",
      "test_roc_auc       0.805278\n",
      "train_roc_auc      0.822448\n",
      "dtype: float64 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "continuousX=X[continuousFeatures].copy()\n",
    "#print(continuousX.head())\n",
    "\n",
    "croppedX = continuousX.copy()\n",
    "croppedX = croppedX.drop([\"thalach\"], axis=1)\n",
    "#print(croppedX.head())\n",
    "\n",
    "\n",
    "\n",
    "nb_results = cross_validate(GaussianNB(), X, Y, cv=5, scoring=[\"accuracy\",\"precision\", \"recall\", \"f1\", \"roc_auc\"], return_train_score=True)\n",
    "results_df = pd.DataFrame(nb_results)\n",
    "print(\"ALL FEATURES \\n\", results_df.mean(), \"\\n\\n\")\n",
    "\n",
    "nb_results = cross_validate(GaussianNB(), continuousX, Y, cv=5, scoring=[\"accuracy\",\"precision\", \"recall\", \"f1\", \"roc_auc\"], return_train_score=True)\n",
    "results_df = pd.DataFrame(nb_results)\n",
    "print(\"CONTINUOUS FEATURES \\n\", results_df.mean(), \"\\n\\n\")\n",
    "\n",
    "nb_results = cross_validate(GaussianNB(), croppedX, Y, cv=5, scoring=[\"accuracy\",\"precision\", \"recall\", \"f1\", \"roc_auc\"], return_train_score=True)\n",
    "results_df = pd.DataFrame(nb_results)\n",
    "print(\"CONTINUOUS FEATURES W/o THALACH \\n\", results_df.mean(), \"\\n\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score_time        0.007607\n",
    "test_accuracy     0.744444\n",
    "test_precision    0.736559\n",
    "test_recall       0.846667\n",
    "test_f1           0.784980\n",
    "test_roc_auc      0.805278"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "\n",
    "\n",
    "# nb_results = cross_validate(LogisticRegression(), standardizedX, Y, cv=5, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"neg_log_loss\"])\n",
    "# results_df = pd.DataFrame(nb_results)\n",
    "# print(\"standardizedX\", \"\\n\", results_df.mean(), \"\\n\\n\")\n",
    "\n",
    "\n",
    "# nb_results = cross_validate(LogisticRegression(), normalizedX, Y, cv=5, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"neg_log_loss\"])\n",
    "# results_df = pd.DataFrame(nb_results)\n",
    "# print(\"normalizedX\",\"\\n\", results_df.mean(), \"\\n\\n\")\n",
    "\n",
    "nb_results = cross_validate(LogisticRegression(), one_hot_standardizedX, Y, cv=5, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"])\n",
    "results_df = pd.DataFrame(nb_results)\n",
    "print(\"one_hot_standardizedX\",\"\\n\", results_df.mean(), \"\\n\\n\")\n",
    "\n",
    "# nb_results = cross_validate(LogisticRegression(), one_hot_normalizedX, Y, cv=5, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"neg_log_loss\"])\n",
    "# results_df = pd.DataFrame(nb_results)\n",
    "# print(\"one_hot_normalizedX\",\"\\n\", results_df.mean(), \"\\n\\n\")\n",
    "\n",
    "param_grid = [    \n",
    "    {\n",
    "    'C' : [0.01,0.1,1,10,100],\n",
    "    }\n",
    "]\n",
    "\n",
    "clf = GridSearchCV(LogisticRegression(), param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"], return_train_score=True, refit=False)\n",
    "clf.fit(one_hot_standardizedX, Y)\n",
    "results_gs = pd.DataFrame(clf.cv_results_)\n",
    "results_gs[['param_C','mean_test_accuracy','mean_test_precision','mean_test_recall','mean_test_f1','mean_test_roc_auc']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = GridSearchCV(KNeighborsClassifier(), {\"n_neighbors\": [1, 3, 5, 7, 9]}, cv=5, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"], return_train_score=True, refit=False)\n",
    "clf.fit(one_hot_standardizedX, Y)\n",
    "results_gs = pd.DataFrame(clf.cv_results_)\n",
    "results_gs[['param_n_neighbors','mean_test_accuracy','mean_test_precision','mean_test_recall','mean_test_f1','mean_test_roc_auc']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>mean_train_accuracy</th>\n",
       "      <th>mean_train_precision</th>\n",
       "      <th>mean_train_recall</th>\n",
       "      <th>mean_train_f1</th>\n",
       "      <th>mean_train_roc_auc</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>mean_test_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.767593</td>\n",
       "      <td>0.797688</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.788497</td>\n",
       "      <td>0.766042</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.746657</td>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.748423</td>\n",
       "      <td>0.718333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.860185</td>\n",
       "      <td>0.844616</td>\n",
       "      <td>0.918333</td>\n",
       "      <td>0.879533</td>\n",
       "      <td>0.912951</td>\n",
       "      <td>0.803704</td>\n",
       "      <td>0.785895</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.835662</td>\n",
       "      <td>0.827222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.945370</td>\n",
       "      <td>0.926742</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.952299</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.748639</td>\n",
       "      <td>0.806667</td>\n",
       "      <td>0.775377</td>\n",
       "      <td>0.739583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.984259</td>\n",
       "      <td>0.977405</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.986012</td>\n",
       "      <td>0.998872</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.767149</td>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.759074</td>\n",
       "      <td>0.732778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0.999074</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998333</td>\n",
       "      <td>0.999163</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.737037</td>\n",
       "      <td>0.770787</td>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.761285</td>\n",
       "      <td>0.737778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  param_max_depth  mean_train_accuracy  mean_train_precision  \\\n",
       "0               1             0.767593              0.797688   \n",
       "1               3             0.860185              0.844616   \n",
       "2               5             0.945370              0.926742   \n",
       "3               7             0.984259              0.977405   \n",
       "4               9             0.999074              1.000000   \n",
       "\n",
       "   mean_train_recall  mean_train_f1  mean_train_roc_auc  mean_test_accuracy  \\\n",
       "0           0.780000       0.788497            0.766042            0.722222   \n",
       "1           0.918333       0.879533            0.912951            0.803704   \n",
       "2           0.980000       0.952299            0.983516            0.740741   \n",
       "3           0.995000       0.986012            0.998872            0.733333   \n",
       "4           0.998333       0.999163            0.999983            0.737037   \n",
       "\n",
       "   mean_test_precision  mean_test_recall  mean_test_f1  mean_test_roc_auc  \n",
       "0             0.746657          0.753333      0.748423           0.718333  \n",
       "1             0.785895          0.893333      0.835662           0.827222  \n",
       "2             0.748639          0.806667      0.775377           0.739583  \n",
       "3             0.767149          0.753333      0.759074           0.732778  \n",
       "4             0.770787          0.753333      0.761285           0.737778  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_DT = X[[\"thalach\", \"oldpeak\", \"ca\", \"exang\"]].copy()\n",
    "\n",
    "clf = GridSearchCV(DecisionTreeClassifier(), {\"max_depth\": [1, 3, 5, 7, 9]}, cv=5, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"], return_train_score=True, refit=False)\n",
    "clf.fit(X, Y)\n",
    "results_gs = pd.DataFrame(clf.cv_results_)\n",
    "results_gs[['param_max_depth','mean_train_accuracy','mean_train_precision','mean_train_recall','mean_train_f1','mean_train_roc_auc', 'mean_test_accuracy','mean_test_precision','mean_test_recall','mean_test_f1','mean_test_roc_auc']]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
