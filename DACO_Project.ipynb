{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for prediction of Heart Disease\n",
    "\n",
    "| Feature         | Description                                                  | Type\n",
    "| ---             | ---                                                          | ---                                    \n",
    "| **age**         | Age                                                          | Real\n",
    "| **sex**         | Sex                                                          | Binary\n",
    "| **cp**          | Chest pain type (4 values)                                   | Nominal\n",
    "| **trestbps**    | Resting blood age                                            | Real\n",
    "| **chol**        | Serum cholesterol (in mg/dl)                                 | Real\n",
    "| **fbs**         | Fasting blood sugar > 120 mg/dl                              | Binary\n",
    "| **restecg**     | Resting electrocardiographic results (values 0,1,2)          | Nominal\n",
    "| **thalach**     | Maximum heart rate achieved                                  | Real\n",
    "| **exang**       | Exercise induced angina                                      | Binary\n",
    "| **oldpeak**     | Oldpeak = ST depression induced by exercise relative to rest | Real\n",
    "| **slope**       | The slope of the peak exercise ST segment                    | Ordered\n",
    "| **ca**          | Number of major vessels (0-3) colored by flouroscopy         | Real\n",
    "| **thal**        | Thal: 3 = normal; 6 = fixed defect; 7 = reversable defect    | Nominal\n",
    "| **target**      | 1 = no disease; 2 = presence of disease                      | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "detail = {\"age\": \"Age\", \"sex\": \"Sex\", \"cp\": \"Chest Pain Type\", \"trestbps\": \"Resting Blood Pressure\",\n",
    "          \"chol\": \"Serum Cholesterol\", \"fbs\": \"Fasting Blood Sugar\", \"restecg\": \"Resting ECG\",\n",
    "          \"thalach\": \"Max Heart Rate\", \"exang\": \"Exercise Induced Angina\", \"oldpeak\": \"Oldpeak\",\n",
    "          \"slope\": \"Slope\", \"ca\": \"Number of major vessels\", \"thal\": \"Thal\", \"target\": \"(0 - no disease, 1 - disease))\"}\n",
    "\n",
    "sns.set_theme(context=\"paper\", font_scale=1.5, style=\"whitegrid\", palette=\"Set2\")\n",
    "\n",
    "data = pd.read_csv(\"heart.dat\", sep=\"\\\\s+\", header=None)\n",
    "data.columns = detail.keys()\n",
    "\n",
    "numericalFeatures = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\", \"ca\"]\n",
    "categoricalFeatures = [\"sex\", \"cp\", \"fbs\", \"restecg\", \"exang\", \"slope\", \"thal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Number of missing values:\", data.isnull().sum().sum())\n",
    "# Check for duplicates\n",
    "print(\"Number of duplicates:\", data.duplicated().sum())\n",
    "\n",
    "X = data.iloc[:, :-1]\n",
    "# Target label is converted to binary (0 - no disease, 1 - disease)\n",
    "Y = data.iloc[:, -1] - 1\n",
    "\n",
    "\n",
    "# Removing extreme OUTLIERS (only 1) (3 * IQR below and above the Q1 and Q3, respectively)\n",
    "Q1 = X[numericalFeatures].quantile(0.25)\n",
    "Q3 = X[numericalFeatures].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Find the outlier row(s)\n",
    "outlier_mask = ((X[numericalFeatures] < (Q1 - 3 * IQR)) | (X[numericalFeatures] > (Q3 + 3 * IQR))).any(axis=1)\n",
    "# Remove the outlier row(s) from X\n",
    "X = X[~outlier_mask].reset_index(drop=True)\n",
    "Y = Y[~outlier_mask].reset_index(drop=True)\n",
    "\n",
    "print(\"Number of outliers removed:\", len(data) - len(X), '\\n')\n",
    "\n",
    "X_numerical = X[numericalFeatures]\n",
    "X_categorical = X[categoricalFeatures]\n",
    "\n",
    "# Apply SCALING only to numerical variables\n",
    "# Standardizing\n",
    "X_stand = X.copy()\n",
    "X_stand[numericalFeatures] = StandardScaler().fit_transform(X_numerical)\n",
    "# Normalizing\n",
    "X_norm = X.copy()\n",
    "X_norm[numericalFeatures] = MinMaxScaler().fit_transform(X_numerical)\n",
    "\n",
    "# ONE HOT ENCODING\n",
    "X_oneHot = pd.get_dummies(X, columns=[\"cp\", \"restecg\", \"slope\", \"thal\"])\n",
    "X_oneHot_stand = pd.get_dummies(X_stand, columns=[\"cp\", \"restecg\", \"slope\", \"thal\"])\n",
    "X_oneHot_norm = pd.get_dummies(X_norm, columns=[\"cp\", \"restecg\", \"slope\", \"thal\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Correlation\n",
    "\n",
    "### Heatmap\n",
    "\n",
    "Although the heatmap should work better with numeric features, categorical binary ones are simple enough that a numeric relationship can apply to its categorical nature.\n",
    "Features used:\n",
    "- **numerical** - age, trestbps, chol, thalach, oldpeak, ca\n",
    "- **categorical** - sex, fbs, exang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (10, 8)\n",
    "vmin = -0.75\n",
    "vmax = 0.75\n",
    "\n",
    "X_heatmap = X[numericalFeatures + [\"sex\", \"fbs\", \"exang\"]].copy()\n",
    "\n",
    "dataCorr = pd.concat([X_heatmap, data[\"target\"]], axis=1).corr()\n",
    "\n",
    "upperHalf_mask = np.tril(np.ones_like(dataCorr, dtype=bool))    # remove bottom left corner\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "plt.title(\"Feature Heatmap\")\n",
    "sns.heatmap(dataCorr, annot=True, linewidths=2,\n",
    "            mask=upperHalf_mask, cmap=\"Spectral_r\", vmin=vmin, vmax=vmax\n",
    ")\n",
    "plt.savefig(f\"plots/heatmap/heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at **trestbps** density distribution and boxplot we can assess that it provides little information for the classification, as the distributions for disease and no disease are pratically overlapping.  \n",
    "\n",
    "A slighter version of the same phenomenon happens for **chol**.  \n",
    "\n",
    "The categorical feature **fbs** also appears to have little effect on separating the two classes, as the normalized bar plot shows the same proportions between target = 2 (diease) and target = 1 (no disease) for both **fbs** classes.  \n",
    "\n",
    "It could be relevant to try out the models without these 3 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=len(X_numerical.columns))\n",
    "pca.fit(X_numerical)\n",
    "X_pca = pca.transform(X_numerical)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# plot no of components vs cumulative explained variance\n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(explained_variance, )\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "# plt.savefig(\"plots/pcVsCEV.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the *Number of Components* against the *Cumulative Explained Variation*, we can see that 3 principal components are useful to explain 100% of the variance, the same number of numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "nPCs = 3\n",
    "pca = PCA(n_components=nPCs)\n",
    "pca_result = pca.fit_transform(X_pca)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Explained Variance: {explained_variance}\")\n",
    "# print(pca.components_)  # feature weight for each pc\n",
    "\n",
    "# Convert it back to a DataFrame\n",
    "pca_df = pd.DataFrame(data=pca_result, columns=[\"PC\" + str(i + 1) for i in range(nPCs)])\n",
    "\n",
    "X_PCAed = pd.concat([pca_df, X.drop(X_numerical.columns, axis=1)], axis=1)\n",
    "\n",
    "print(X_PCAed.head())\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(pca_df['PC1'], pca_df['PC2'], pca_df['PC3'])\n",
    "\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "def naiveBayes_implement(nb_model, X_bayes, stringX):\n",
    "    perfMetrics = [\"recall\", \"f1\", \"roc_auc\"]\n",
    "    nb_results = cross_validate(nb_model, X_bayes, Y, cv=5, scoring=perfMetrics, return_train_score=True)\n",
    "    results_df = pd.DataFrame(nb_results)\n",
    "    results_df.drop(columns=['fit_time', 'score_time'], inplace=True)  # Exclude fit_time and score_time\n",
    "    print(f\"{stringX} \\n\", results_df.mean(), \"\\n\\n\")\n",
    "\n",
    "\n",
    "# Drop ca\n",
    "X_noCa = X_numerical.copy().drop([\"ca\"], axis=1)\n",
    "# Drop trestbpse col\n",
    "X_noTrestBpsChol = X_numerical.copy().drop([\"trestbps\", \"chol\"], axis=1)\n",
    "# Drop thalach\n",
    "X_noThalach = X_numerical.copy().drop([\"thalach\"], axis=1)\n",
    "# Drop fbs\n",
    "X_noFbs = X_categorical.copy().drop([\"fbs\"], axis=1)\n",
    "# PCAed without categorical\n",
    "X_PCAed_noCategorical = X_PCAed.copy().drop(X_categorical.columns, axis=1)\n",
    "\n",
    "naiveBayes_implement(GaussianNB(), X, \"X\")\n",
    "naiveBayes_implement(GaussianNB(), X_noCa, \"X_noCa\")\n",
    "naiveBayes_implement(GaussianNB(), X_numerical, \"X_numerical\")\n",
    "naiveBayes_implement(GaussianNB(), X_noTrestBpsChol, \"X_noTrestBpsChol\")\n",
    "naiveBayes_implement(GaussianNB(), X_noThalach, \"X_noThalach\")\n",
    "naiveBayes_implement(CategoricalNB(), X_categorical, \"X_categorical\")\n",
    "naiveBayes_implement(CategoricalNB(), X_noFbs, \"X_noFbs\")\n",
    "naiveBayes_implement(GaussianNB(), X_PCAed, \"X_PCAed\")\n",
    "naiveBayes_implement(GaussianNB(), X_PCAed_noCategorical, \"X_PCAed_noCategorical\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid = [    \n",
    "    {\n",
    "    'C' : [0.01,0.1,1,10,100],\n",
    "    }\n",
    "]\n",
    "\n",
    "clf = GridSearchCV(LogisticRegression(), param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"], return_train_score=True, refit=False)\n",
    "clf.fit(X_oneHot_stand, Y)\n",
    "results_gs = pd.DataFrame(clf.cv_results_)\n",
    "results_gs[['param_C','mean_train_recall','mean_train_f1', 'mean_train_roc_auc', 'mean_test_recall','mean_test_f1', 'mean_test_roc_auc']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Calcular o erro para K's entre 1 e 30\n",
    "knn_model = GridSearchCV(KNeighborsClassifier(), {\"n_neighbors\": range(1, 30, 30)}, cv=5, scoring=[\"f1\"], return_train_score=False, refit=False)\n",
    "knn_model.fit(X_oneHot, Y)\n",
    "results_knn = pd.DataFrame(knn_model.cv_results_)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(results_knn['param_n_neighbors'], results_knn['mean_test_f1'], color='red', linestyle='dashed', marker='o', markerfacecolor='blue', markersize=10)\n",
    "plt.title('F1 Error Rate (K Value)')\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('F1 Error')\n",
    "plt.show()\n",
    "\n",
    "clf = GridSearchCV(KNeighborsClassifier(), {\"n_neighbors\": [2, 6, 8, 11]}, cv=5, scoring=[\"recall\", \"f1\", \"roc_auc\"], return_train_score=True, refit=False)\n",
    "clf.fit(X_oneHot, Y)\n",
    "results_gs = pd.DataFrame(clf.cv_results_)\n",
    "\n",
    "# Print the metrics table\n",
    "print(\"Metrics Table:\")\n",
    "print(results_gs[['param_n_neighbors', 'mean_test_recall', 'mean_test_f1', 'mean_test_roc_auc']])\n",
    "print(results_gs[['param_n_neighbors', 'mean_train_recall', 'mean_train_f1', 'mean_train_roc_auc']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>mean_train_recall</th>\n",
       "      <th>mean_train_f1</th>\n",
       "      <th>mean_train_roc_auc</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>mean_test_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.810417</td>\n",
       "      <td>0.837568</td>\n",
       "      <td>0.911711</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.729519</td>\n",
       "      <td>0.807217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.810417</td>\n",
       "      <td>0.837568</td>\n",
       "      <td>0.911711</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.732620</td>\n",
       "      <td>0.811245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.810417</td>\n",
       "      <td>0.837568</td>\n",
       "      <td>0.911711</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.729519</td>\n",
       "      <td>0.807217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.902083</td>\n",
       "      <td>0.939268</td>\n",
       "      <td>0.982127</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.737590</td>\n",
       "      <td>0.766159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.897917</td>\n",
       "      <td>0.933837</td>\n",
       "      <td>0.980791</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.706881</td>\n",
       "      <td>0.749397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.885417</td>\n",
       "      <td>0.913812</td>\n",
       "      <td>0.974278</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.716786</td>\n",
       "      <td>0.783046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.984000</td>\n",
       "      <td>0.998792</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.717196</td>\n",
       "      <td>0.752203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.964641</td>\n",
       "      <td>0.996904</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.707278</td>\n",
       "      <td>0.756317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.908333</td>\n",
       "      <td>0.927459</td>\n",
       "      <td>0.988256</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.713725</td>\n",
       "      <td>0.790704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  param_max_depth param_min_samples_split  mean_train_recall  mean_train_f1  \\\n",
       "0               3                       2           0.810417       0.837568   \n",
       "1               3                       4           0.810417       0.837568   \n",
       "2               3                       8           0.810417       0.837568   \n",
       "3               5                       2           0.902083       0.939268   \n",
       "4               5                       4           0.897917       0.933837   \n",
       "5               5                       8           0.885417       0.913812   \n",
       "6               7                       2           0.968750       0.984000   \n",
       "7               7                       4           0.941667       0.964641   \n",
       "8               7                       8           0.908333       0.927459   \n",
       "\n",
       "   mean_train_roc_auc  mean_test_recall  mean_test_f1  mean_test_roc_auc  \n",
       "0            0.911711          0.700000      0.729519           0.807217  \n",
       "1            0.911711          0.708333      0.732620           0.811245  \n",
       "2            0.911711          0.700000      0.729519           0.807217  \n",
       "3            0.982127          0.716667      0.737590           0.766159  \n",
       "4            0.980791          0.683333      0.706881           0.749397  \n",
       "5            0.974278          0.700000      0.716786           0.783046  \n",
       "6            0.998792          0.716667      0.717196           0.752203  \n",
       "7            0.996904          0.683333      0.707278           0.756317  \n",
       "8            0.988256          0.700000      0.713725           0.790704  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "param_grid = {\"max_depth\": [3, 5, 7], \"min_samples_split\": [2, 4, 8]}\n",
    "\n",
    "clf = GridSearchCV(DecisionTreeClassifier(), param_grid=param_grid, cv=5, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"], return_train_score=True, refit=False)\n",
    "clf.fit(X_oneHot, Y)\n",
    "results_gs = pd.DataFrame(clf.cv_results_)\n",
    "results_gs[['param_max_depth','param_min_samples_split','mean_train_recall', 'mean_train_f1','mean_train_roc_auc', 'mean_test_recall', 'mean_test_f1','mean_test_roc_auc']]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
