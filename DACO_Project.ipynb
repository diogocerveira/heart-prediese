{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of data types, \"ordered\" data refers to ordinal data.\n",
    "Ordinal data is a type of categorical data with an order (or rank).\n",
    "The order of these values is significant and typically represents some sort of hierarchy.\n",
    "For example, ratings data (like \"poor\", \"average\", \"good\", \"excellent\") is ordinal\n",
    "because there is a clear order to the categories.\n",
    "\n",
    "| Feature         | Description                                                  | Type\n",
    "| ---             | ---                                                          | ---                                    \n",
    "| **age**         | Age                                                          | Real\n",
    "| **sex**         | Sex                                                          | Binary\n",
    "| **cp**          | Chest pain type (4 values)                                   | Nominal\n",
    "| **trestbps**    | Resting blood age                                            | Real\n",
    "| **chol**        | Serum cholesterol (in mg/dl)                                 | Real\n",
    "| **fbs**         | Fasting blood sugar > 120 mg/dl                              | Binary\n",
    "| **restecg**     | Resting electrocardiographic results (values 0,1,2)          | Nominal\n",
    "| **thalach**     | Maximum heart rate achieved                                  | Real\n",
    "| **exang**       | Exercise induced angina                                      | Binary\n",
    "| **oldpeak**     | Oldpeak = ST depression induced by exercise relative to rest | Real\n",
    "| **slope**       | The slope of the peak exercise ST segment                    | Ordered\n",
    "| **ca**          | Number of major vessels (0-3) colored by flouroscopy         | Real\n",
    "| **thal**        | Thal: 3 = normal; 6 = fixed defect; 7 = reversable defect    | Nominal\n",
    "| **target**      | 1 = no disease; 2 = presence of disease                      | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "detail = {\"age\": \"Age\", \"sex\": \"Sex\", \"cp\": \"Chest Pain Type\", \"trestbps\": \"Resting Blood Pressure\",\n",
    "          \"chol\": \"Serum Cholesterol\", \"fbs\": \"Fasting Blood Sugar\", \"restecg\": \"Resting ECG\",\n",
    "          \"thalach\": \"Max Heart Rate\", \"exang\": \"Exercise Induced Angina\", \"oldpeak\": \"Oldpeak\",\n",
    "          \"slope\": \"Slope\", \"ca\": \"Number of major vessels\", \"thal\": \"Thal\", \"target\": \"(0 - no disease, 1 - disease))\"}\n",
    "\n",
    "sns.set_theme(context=\"paper\", font_scale=1.5, style=\"whitegrid\", palette=\"Set2\")\n",
    "\n",
    "data = pd.read_csv(\"heart.dat\", sep=\"\\\\s+\", header=None)\n",
    "data.columns = detail.keys()\n",
    "\n",
    "numericFeatures = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\", \"ca\"]\n",
    "categoricalFeatures = [\"sex\", \"cp\", \"fbs\", \"restecg\", \"exang\", \"slope\", \"thal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Number of missing values:\", data.isnull().sum().sum(), \"\\n\")\n",
    "# Check for duplicates\n",
    "print(\"Number of duplicates:\", data.duplicated().sum(), \"\\n\")\n",
    "\n",
    "X = data.iloc[:, :-1]\n",
    "Y = data.iloc[:, -1]\n",
    "\n",
    "# print(data.describe())\n",
    "\n",
    "# print(X[continuousFeatures].head(), \"\\n\")\n",
    "# Apply scaler only to continuous variables\n",
    "X[numericFeatures] = StandardScaler().fit_transform(X[numericFeatures])\n",
    "#X[continuousFeatures] = MinMaxScaler().fit_transform(X[continuousFeatures])\n",
    "\n",
    "normalizedX = X.copy()\n",
    "normalizedX[continuousFeatures] = MinMaxScaler().fit_transform(X[continuousFeatures])\n",
    "\n",
    "X_oneHot = pd.get_dummies(X, columns=[\"cp\", \"restecg\", \"slope\", \"thal\"])\n",
    "\n",
    "one_hot_X = pd.get_dummies(X, columns=[\"cp\", \"restecg\", \"slope\", \"thal\"])\n",
    "one_hot_standardizedX = pd.get_dummies(standardizedX, columns=[\"cp\", \"restecg\", \"slope\", \"thal\"])\n",
    "one_hot_normalizedX = pd.get_dummies(normalizedX, columns=[\"cp\", \"restecg\", \"slope\", \"thal\"])\n",
    "\n",
    "print(\"one hot\", one_hot_X.head(), \"\\n\")\n",
    "print(\"one hot standard\", one_hot_standardizedX.head(), \"\\n\")\n",
    "print(\"one hot norm\", one_hot_normalizedX.head(), \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Correlation\n",
    "\n",
    "## Heatmap\n",
    "\n",
    "Although the heatmap should work better with numeric features, categorical binary ones also are simple enough that a numeric relationship can also apply to its categorical nature.\n",
    "Features used:\n",
    "- **numerical** - age, trestbps, chol, thalach, oldpeak, ca\n",
    "- **categorical** - sex, fbs, exang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (10, 8)\n",
    "vmin = -0.75\n",
    "vmax = 0.75\n",
    "\n",
    "X_heatmap = X[numericFeatures + [\"sex\", \"fbs\", \"exang\"]]\n",
    "\n",
    "dataCorr = pd.concat([X_heatmap, data[\"target\"]], axis=1).corr()\n",
    "\n",
    "upperHalf_mask = np.tril(np.ones_like(dataCorr, dtype=bool))    # remove bottom left corner\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "plt.title(\"Feature Heatmap\")\n",
    "sns.heatmap(dataCorr, annot=True, linewidths=2,\n",
    "            mask=upperHalf_mask, cmap=\"Spectral_r\", vmin=vmin, vmax=vmax\n",
    ")\n",
    "plt.savefig(f\"plots/heatmap/heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features like **fbs**, **chol** and **trestbps** seem uninteresting for target prediction. Before trying the models without them, let's try removing the outliers and see if anything changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_heatmap = X[numericFeatures]\n",
    "\n",
    "Q1 = X_heatmap.quantile(0.25)\n",
    "Q3 = X_heatmap.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Removing outliers 1.5 * IQR below and above the Q1 and Q3, respectively\n",
    "X_heatmap_noOut =  X_heatmap[~((X_heatmap < (Q1 - 1.5 * IQR)) | (X_heatmap > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "X_heatmap_noOut = pd.concat([X_heatmap_noOut, X[[\"sex\", \"fbs\", \"exang\"]]], axis=1)\n",
    "\n",
    "dataCorr_noOut = pd.concat([X_heatmap_noOut, data[\"target\"]], axis=1).corr()\n",
    "\n",
    "upperHalf_mask = np.tril(np.ones_like(dataCorr_noOut, dtype=bool))\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "plt.title(\"No Outliers Feature Heatmap\")\n",
    "sns.heatmap(dataCorr_noOut, annot=True, linewidths=2,\n",
    "            mask=upperHalf_mask, cmap=\"Spectral_r\", vmin=vmin, vmax=vmax\n",
    ")\n",
    "plt.savefig(f\"plots/heatmap/heatmap_noOutliers.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the difference between heatmaps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference between the two correlation matrices\n",
    "heatmap_diff = dataCorr_noOut - dataCorr\n",
    "\n",
    "# Plot the difference heatmap\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "plt.title(\"Difference between Heatmaps\")\n",
    "sns.heatmap(heatmap_diff, annot=True, linewidths=2, mask=upperHalf_mask, cmap=\"coolwarm\")\n",
    "plt.savefig(f\"plots/heatmap/heatmap_diff.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature **trestbps** was the most affected by outlier removal, as the correlation decrease 0.79 from 0.16. This could say the outliers were relevant cases to consider, but we can look at its density distribution and boxplot to assess that it provides little information for the classification, as the distributions for disease and no disease are pratically overlapping.  \n",
    "\n",
    "A slighter version of the same phenomenon happens for **chol**.  \n",
    "\n",
    "The categorical feature **fbs** also appears to have little effect on separating the two classes, as the normalized bar plot shows the same proportions between target = 2 (diease) and target = 1 (no disease) for both **fbs** classes.  \n",
    "\n",
    "It should be relevant to try out the models without these 3 features.\n",
    "\n",
    "Also, removing features like **age** for Naive Bayes could in theory provide better results, as it is not strongly correlate with the target and but correlates a bit with many other features. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "[PCA Explanation](https://builtin.com/data-science/step-step-explanation-principal-component-analysis)  \n",
    "\n",
    "GitHub Copilot: Advantages of using PCA for dimensionality reduction:\n",
    "\n",
    "1. **Removes Correlated Features**: In the real world, features are often correlated. PCA allows you to identify the most important features of your dataset, reducing it to a smaller set of uncorrelated features, known as principal components.\n",
    "\n",
    "2. **Improves Algorithm Performance**: With fewer features, the performance of a machine learning algorithm can improve. It can also reduce overfitting.\n",
    "\n",
    "3. **Reduces Overfitting**: By reducing the dimensionality of your feature space, you're less likely to overfit your model.\n",
    "\n",
    "4. **Improves Visualization**: It's hard to visualize high dimensional data. PCA transforms a high dimensional data set to 2 or 3 dimensions so we can plot and understand data better.\n",
    "\n",
    "Disadvantages of using PCA:\n",
    "\n",
    "1. **Independent variables become less interpretable**: After implementing PCA, your original features will turn into Principal Components. Principal Components are the linear combination of your original features. Principal Components are not as readable and interpretable as original features.\n",
    "\n",
    "2. **Data standardization is must before PCA**: You must standardize your data before implementing PCA, otherwise PCA will not be able to find the optimal Principal Components.\n",
    "\n",
    "3. **Information Loss**: Although principal components attempt to retain as much information as possible, some information is lost when reducing dimensions, which can potentially degrade the performance of your machine learning model.\n",
    "\n",
    "4. **Doesn't handle non-linear features well**: PCA assumes that the principal components are a linear combination of the original features. If this assumption is not true, PCA may not give you the results you're looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_pca = X[numericFeatures]\n",
    "\n",
    "pca = PCA(n_components=len(X_pca.columns))\n",
    "pca_result = pca.fit_transform(X_pca)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# plot no of components vs cumulative explained variance\n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(explained_variance, )\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the *Number of Components* against the *Cumulative Explained Variation*, we can see that 5 principal components are useful to explain 100% of the variance, the same number of numeric features. PCA won't provide the benefit of reducing dimensionality of the dataset.\n",
    "\n",
    "Copilot:\n",
    "\n",
    "That said, PCA can still be useful in this case for other reasons:\n",
    "\n",
    "1. **Feature Independence**: The PCs are linearly independent of each other, which can help with certain types of models that assume feature independence (like linear regression).\n",
    "\n",
    "2. **Interpretability**: PCs can sometimes be interpreted in terms of the original features, which can provide insights into the structure of your data.\n",
    "\n",
    "3. **Noise Reduction**: PCA can help to reduce noise in your data by focusing on the directions of maximum variance and ignoring smaller, potentially noisy fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "nPCs = 5\n",
    "pca = PCA(n_components=nPCs)\n",
    "pca_result = pca.fit_transform(X_pca)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Explained Variance: {explained_variance}\")\n",
    "# print(pca.components_)  # feature weight for each pc\n",
    "\n",
    "# Convert it back to a DataFrame\n",
    "pca_df = pd.DataFrame(data=pca_result, columns=[\"PC\" + str(i + 1) for i in range(nPCs)])\n",
    "\n",
    "\n",
    "X_oneHot_PCAed = pd.concat([pca_df, X_oneHot.drop(X_oneHot.columns, axis=1)], axis=1)\n",
    "\n",
    "# print(X_oneHot_PCAed.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create empty lists to store the evaluation metrics\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "\n",
    "continuousX=X[continuousFeatures].copy()\n",
    "#print(continuousX.head())\n",
    "\n",
    "croppedX = continuousX.copy()\n",
    "croppedX = croppedX.drop([\"thalach\"], axis=1)\n",
    "#print(croppedX.head())\n",
    "\n",
    "\n",
    "\n",
    "nb_results = cross_validate(GaussianNB(), X, Y, cv=5, scoring=[\"accuracy\",\"precision\", \"recall\", \"f1\", \"roc_auc\"], return_train_score=True)\n",
    "results_df = pd.DataFrame(nb_results)\n",
    "print(\"ALL FEATURES \\n\", results_df.mean(), \"\\n\\n\")\n",
    "\n",
    "nb_results = cross_validate(GaussianNB(), continuousX, Y, cv=5, scoring=[\"accuracy\",\"precision\", \"recall\", \"f1\", \"roc_auc\"], return_train_score=True)\n",
    "results_df = pd.DataFrame(nb_results)\n",
    "print(\"CONTINUOUS FEATURES \\n\", results_df.mean(), \"\\n\\n\")\n",
    "\n",
    "nb_results = cross_validate(GaussianNB(), croppedX, Y, cv=5, scoring=[\"accuracy\",\"precision\", \"recall\", \"f1\", \"roc_auc\"], return_train_score=True)\n",
    "results_df = pd.DataFrame(nb_results)\n",
    "print(\"CONTINUOUS FEATURES W/o THALACH \\n\", results_df.mean(), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score_time        0.007607\n",
    "test_accuracy     0.744444\n",
    "test_precision    0.736559\n",
    "test_recall       0.846667\n",
    "test_f1           0.784980\n",
    "test_roc_auc      0.805278"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# nb_results = cross_validate(LogisticRegression(), standardizedX, Y, cv=5, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"neg_log_loss\"])\n",
    "# results_df = pd.DataFrame(nb_results)\n",
    "# print(\"standardizedX\", \"\\n\", results_df.mean(), \"\\n\\n\")\n",
    "\n",
    "\n",
    "# Sistema\n",
    "\n",
    "nb_results = cross_validate(LogisticRegression(), one_hot_standardizedX, Y, cv=5, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"])\n",
    "results_df = pd.DataFrame(nb_results)\n",
    "print(\"one_hot_standardizedX\",\"\\n\", results_df.mean(), \"\\n\\n\")\n",
    "\n",
    "# nb_results = cross_validate(LogisticRegression(), one_hot_normalizedX, Y, cv=5, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"neg_log_loss\"])\n",
    "# results_df = pd.DataFrame(nb_results)\n",
    "# print(\"one_hot_normalizedX\",\"\\n\", results_df.mean(), \"\\n\\n\")\n",
    "\n",
    "param_grid = [    \n",
    "    {\n",
    "    'C' : [0.01,0.1,1,10,100],\n",
    "    }\n",
    "]\n",
    "\n",
    "clf = GridSearchCV(LogisticRegression(), param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"], return_train_score=True, refit=False)\n",
    "clf.fit(one_hot_standardizedX, Y)\n",
    "results_gs = pd.DataFrame(clf.cv_results_)\n",
    "results_gs[['param_C','mean_test_accuracy','mean_test_precision','mean_test_recall','mean_test_f1','mean_test_roc_auc']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Ensure one_hot_standardizedX and Y are properly defined\n",
    "\n",
    "param_grid = {\"n_neighbors\": np.linspace(1, 30).astype(int)}\n",
    "\n",
    "# Choose a single scoring metric (e.g., \"accuracy\") for refit\n",
    "refit_metric = \"accuracy\"\n",
    "\n",
    "clf = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"],\n",
    "    return_train_score=True,\n",
    "    refit=refit_metric\n",
    ")\n",
    "\n",
    "clf.fit(one_hot_standardizedX, Y)\n",
    "\n",
    "results_gs = pd.DataFrame(clf.cv_results_)\n",
    "results_subset = results_gs[\n",
    "    [\"param_n_neighbors\", f\"mean_test_{refit_metric}\", f\"mean_train_{refit_metric}\"]\n",
    "]\n",
    "print(results_subset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_DT = X[[\"thalach\", \"oldpeak\", \"ca\", \"exang\"]].copy()\n",
    "\n",
    "clf = GridSearchCV(DecisionTreeClassifier(), {\"max_depth\": [1, 3, 5, 7, 9]}, cv=5, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"], return_train_score=True, refit=False)\n",
    "clf.fit(X, Y)\n",
    "results_gs = pd.DataFrame(clf.cv_results_)\n",
    "results_gs[['param_max_depth','mean_train_accuracy','mean_train_precision','mean_train_recall','mean_train_f1','mean_train_roc_auc', 'mean_test_accuracy','mean_test_precision','mean_test_recall','mean_test_f1','mean_test_roc_auc']]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
