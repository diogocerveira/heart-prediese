{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for prediction of Heart Disease\n",
    "\n",
    "| Feature         | Description                                                  | Type\n",
    "| ---             | ---                                                          | ---                                    \n",
    "| **age**         | Age                                                          | Real\n",
    "| **sex**         | Sex                                                          | Binary\n",
    "| **cp**          | Chest pain type (4 values)                                   | Nominal\n",
    "| **trestbps**    | Resting blood age                                            | Real\n",
    "| **chol**        | Serum cholesterol (in mg/dl)                                 | Real\n",
    "| **fbs**         | Fasting blood sugar > 120 mg/dl                              | Binary\n",
    "| **restecg**     | Resting electrocardiographic results (values 0,1,2)          | Nominal\n",
    "| **thalach**     | Maximum heart rate achieved                                  | Real\n",
    "| **exang**       | Exercise induced angina                                      | Binary\n",
    "| **oldpeak**     | Oldpeak = ST depression induced by exercise relative to rest | Real\n",
    "| **slope**       | The slope of the peak exercise ST segment                    | Ordered\n",
    "| **ca**          | Number of major vessels (0-3) colored by flouroscopy         | Real\n",
    "| **thal**        | Thal: 3 = normal; 6 = fixed defect; 7 = reversable defect    | Nominal\n",
    "| **target**      | 1 = no disease; 2 = presence of disease                      | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "detail = {\"age\": \"Age\", \"sex\": \"Sex\", \"cp\": \"Chest Pain Type\", \"trestbps\": \"Resting Blood Pressure\",\n",
    "          \"chol\": \"Serum Cholesterol\", \"fbs\": \"Fasting Blood Sugar\", \"restecg\": \"Resting ECG\",\n",
    "          \"thalach\": \"Max Heart Rate\", \"exang\": \"Exercise Induced Angina\", \"oldpeak\": \"Oldpeak\",\n",
    "          \"slope\": \"Slope\", \"ca\": \"Number of major vessels\", \"thal\": \"Thal\", \"target\": \"(0 - no disease, 1 - disease))\"}\n",
    "\n",
    "sns.set_theme(context=\"paper\", font_scale=1.5, style=\"whitegrid\", palette=\"Set2\")\n",
    "\n",
    "data = pd.read_csv(\"heart.dat\", sep=\"\\\\s+\", header=None)\n",
    "data.columns = detail.keys()\n",
    "\n",
    "numericalFeatures = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\", \"ca\"]\n",
    "categoricalFeatures = [\"sex\", \"cp\", \"fbs\", \"restecg\", \"exang\", \"slope\", \"thal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Number of missing values:\", data.isnull().sum().sum(), \"\\n\")\n",
    "# Check for duplicates\n",
    "print(\"Number of duplicates:\", data.duplicated().sum(), \"\\n\")\n",
    "\n",
    "X = data.iloc[:, :-1]\n",
    "Y = data.iloc[:, -1]\n",
    "\n",
    "# print(data.describe())\n",
    "\n",
    "# print(X[numericalFeatures].head(), \"\\n\")\n",
    "# Apply scaler only to numerical variables\n",
    "X[numericalFeatures] = StandardScaler().fit_transform(X[numericalFeatures])\n",
    "#X[numericalFeatures] = MinMaxScaler().fit_transform(X[numericalFeatures])\n",
    "\n",
    "normalizedX = X.copy()\n",
    "normalizedX[numericalFeatures] = MinMaxScaler().fit_transform(X[numericalFeatures])\n",
    "\n",
    "X_oneHot = pd.get_dummies(X, columns=[\"cp\", \"restecg\", \"slope\", \"thal\"])\n",
    "\n",
    "one_hot_X = pd.get_dummies(X, columns=[\"cp\", \"restecg\", \"slope\", \"thal\"])\n",
    "one_hot_standardizedX = pd.get_dummies(standardizedX, columns=[\"cp\", \"restecg\", \"slope\", \"thal\"])\n",
    "one_hot_normalizedX = pd.get_dummies(normalizedX, columns=[\"cp\", \"restecg\", \"slope\", \"thal\"])\n",
    "\n",
    "print(\"one hot\", one_hot_X.head(), \"\\n\")\n",
    "print(\"one hot standard\", one_hot_standardizedX.head(), \"\\n\")\n",
    "print(\"one hot norm\", one_hot_normalizedX.head(), \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Correlation\n",
    "\n",
    "## Heatmap\n",
    "\n",
    "Although the heatmap should work better with numeric features, categorical binary ones also are simple enough that a numeric relationship can also apply to its categorical nature.\n",
    "Features used:\n",
    "- **numerical** - age, trestbps, chol, thalach, oldpeak, ca\n",
    "- **categorical** - sex, fbs, exang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (10, 8)\n",
    "vmin = -0.75\n",
    "vmax = 0.75\n",
    "\n",
    "X_heatmap = X[numericalFeatures + [\"sex\", \"fbs\", \"exang\"]]\n",
    "\n",
    "dataCorr = pd.concat([X_heatmap, data[\"target\"]], axis=1).corr()\n",
    "\n",
    "upperHalf_mask = np.tril(np.ones_like(dataCorr, dtype=bool))    # remove bottom left corner\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "plt.title(\"Feature Heatmap\")\n",
    "sns.heatmap(dataCorr, annot=True, linewidths=2,\n",
    "            mask=upperHalf_mask, cmap=\"Spectral_r\", vmin=vmin, vmax=vmax\n",
    ")\n",
    "plt.savefig(f\"plots/heatmap/heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features like **fbs**, **chol** and **trestbps** seem uninteresting for target prediction. Before trying the models without them, let's try removing the outliers and see if anything changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_heatmap = X[numericalFeatures]\n",
    "\n",
    "Q1 = X_heatmap.quantile(0.25)\n",
    "Q3 = X_heatmap.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Removing outliers 1.5 * IQR below and above the Q1 and Q3, respectively\n",
    "X_heatmap_noOut =  X_heatmap[~((X_heatmap < (Q1 - 1.5 * IQR)) | (X_heatmap > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "X_heatmap_noOut = pd.concat([X_heatmap_noOut, X[[\"sex\", \"fbs\", \"exang\"]]], axis=1)\n",
    "\n",
    "dataCorr_noOut = pd.concat([X_heatmap_noOut, data[\"target\"]], axis=1).corr()\n",
    "\n",
    "upperHalf_mask = np.tril(np.ones_like(dataCorr_noOut, dtype=bool))\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "plt.title(\"No Outliers Feature Heatmap\")\n",
    "sns.heatmap(dataCorr_noOut, annot=True, linewidths=2,\n",
    "            mask=upperHalf_mask, cmap=\"Spectral_r\", vmin=vmin, vmax=vmax\n",
    ")\n",
    "plt.savefig(f\"plots/heatmap/heatmap_noOutliers.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the difference between heatmaps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference between the two correlation matrices\n",
    "heatmap_diff = dataCorr_noOut - dataCorr\n",
    "\n",
    "# Plot the difference heatmap\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "plt.title(\"Difference between Heatmaps\")\n",
    "sns.heatmap(heatmap_diff, annot=True, linewidths=2, mask=upperHalf_mask, cmap=\"coolwarm\")\n",
    "plt.savefig(f\"plots/heatmap/heatmap_diff.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature **trestbps** was the most affected by outlier removal, as the correlation decrease 0.79 from 0.16. This could say the outliers were relevant cases to consider, but we can look at its density distribution and boxplot to assess that it provides little information for the classification, as the distributions for disease and no disease are pratically overlapping.  \n",
    "\n",
    "A slighter version of the same phenomenon happens for **chol**.  \n",
    "\n",
    "The categorical feature **fbs** also appears to have little effect on separating the two classes, as the normalized bar plot shows the same proportions between target = 2 (diease) and target = 1 (no disease) for both **fbs** classes.  \n",
    "\n",
    "It should be relevant to try out the models without these 3 features.\n",
    "\n",
    "Also, removing features like **age** for Naive Bayes could in theory provide better results, as it is not strongly correlate with the target and but correlates a bit with many other features. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "[PCA Explanation](https://builtin.com/data-science/step-step-explanation-principal-component-analysis)  \n",
    "\n",
    "GitHub Copilot: Advantages of using PCA for dimensionality reduction:\n",
    "\n",
    "1. **Removes Correlated Features**: In the real world, features are often correlated. PCA allows you to identify the most important features of your dataset, reducing it to a smaller set of uncorrelated features, known as principal components.\n",
    "\n",
    "2. **Improves Algorithm Performance**: With fewer features, the performance of a machine learning algorithm can improve. It can also reduce overfitting.\n",
    "\n",
    "3. **Reduces Overfitting**: By reducing the dimensionality of your feature space, you're less likely to overfit your model.\n",
    "\n",
    "4. **Improves Visualization**: It's hard to visualize high dimensional data. PCA transforms a high dimensional data set to 2 or 3 dimensions so we can plot and understand data better.\n",
    "\n",
    "Disadvantages of using PCA:\n",
    "\n",
    "1. **Independent variables become less interpretable**: After implementing PCA, your original features will turn into Principal Components. Principal Components are the linear combination of your original features. Principal Components are not as readable and interpretable as original features.\n",
    "\n",
    "2. **Data standardization is must before PCA**: You must standardize your data before implementing PCA, otherwise PCA will not be able to find the optimal Principal Components.\n",
    "\n",
    "3. **Information Loss**: Although principal components attempt to retain as much information as possible, some information is lost when reducing dimensions, which can potentially degrade the performance of your machine learning model.\n",
    "\n",
    "4. **Doesn't handle non-linear features well**: PCA assumes that the principal components are a linear combination of the original features. If this assumption is not true, PCA may not give you the results you're looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_pca = X[numericalFeatures]\n",
    "\n",
    "pca = PCA(n_components=len(X_pca.columns))\n",
    "pca_result = pca.fit_transform(X_pca)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# plot no of components vs cumulative explained variance\n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(explained_variance, )\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the *Number of Components* against the *Cumulative Explained Variation*, we can see that 5 principal components are useful to explain 100% of the variance, the same number of numeric features. PCA won't provide the benefit of reducing dimensionality of the dataset.\n",
    "\n",
    "Copilot:\n",
    "\n",
    "That said, PCA can still be useful in this case for other reasons:\n",
    "\n",
    "1. **Feature Independence**: The PCs are linearly independent of each other, which can help with certain types of models that assume feature independence (like linear regression).\n",
    "\n",
    "2. **Interpretability**: PCs can sometimes be interpreted in terms of the original features, which can provide insights into the structure of your data.\n",
    "\n",
    "3. **Noise Reduction**: PCA can help to reduce noise in your data by focusing on the directions of maximum variance and ignoring smaller, potentially noisy fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance: [0.3447508  0.18632201 0.15375034 0.12203783 0.11928476]\n"
     ]
    }
   ],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "nPCs = 5\n",
    "pca = PCA(n_components=nPCs)\n",
    "pca_result = pca.fit_transform(X_pca)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Explained Variance: {explained_variance}\")\n",
    "# print(pca.components_)  # feature weight for each pc\n",
    "\n",
    "# Convert it back to a DataFrame\n",
    "pca_df = pd.DataFrame(data=pca_result, columns=[\"PC\" + str(i + 1) for i in range(nPCs)])\n",
    "\n",
    "\n",
    "X_oneHot_PCAed = pd.concat([pca_df, X_oneHot.drop(X_oneHot.columns, axis=1)], axis=1)\n",
    "\n",
    "# print(X_oneHot_PCAed.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create empty lists to store the evaluation metrics\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "\n",
    "continuousX=X[numericalFeatures].copy()\n",
    "print(continuousX.head())\n",
    "\n",
    "croppedX = continuousX.copy()\n",
    "croppedX = croppedX.drop([\"thalach\"], axis=1)\n",
    "print(croppedX.head())\n",
    "\n",
    "\n",
    "\n",
    "nb_results = cross_validate(GaussianNB(), X, Y, cv=5, scoring=[\"accuracy\",\"precision\", \"recall\", \"f1\", \"roc_auc\"], return_train_score=True)\n",
    "results_df = pd.DataFrame(nb_results)\n",
    "print(results_df.mean())\n",
    "\n",
    "nb_results = cross_validate(GaussianNB(), continuousX, Y, cv=5, scoring=[\"accuracy\",\"precision\", \"recall\", \"f1\", \"roc_auc\"], return_train_score=True)\n",
    "results_df = pd.DataFrame(nb_results)\n",
    "print(results_df.mean())\n",
    "\n",
    "nb_results = cross_validate(GaussianNB(), croppedX, Y, cv=5, scoring=[\"accuracy\",\"precision\", \"recall\", \"f1\", \"roc_auc\"], return_train_score=True)\n",
    "results_df = pd.DataFrame(nb_results)\n",
    "print(results_df.mean())\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    # pos_label referes to the HD presence class (the positive class)\n",
    "    pos_label = 1\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, pos_label=pos_label)\n",
    "    recall = recall_score(y_test, y_pred, pos_label=pos_label)\n",
    "    f1 = f1_score(y_test, y_pred, pos_label=pos_label)\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Append the evaluation metrics to the respective lists\n",
    "    accuracy_list.append(accuracy)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(f1)\n",
    "\n",
    "    # Print the evaluation metrics for the current iteration\n",
    "    # print(\"Confusion Matrix:\")\n",
    "    # print(pd.DataFrame(cm, columns=[\"Predicted HD Absence\", \"Predicted HD Presence\"], index=[\"Actual HD Absence\", \"Actual HD Presence\"]))\n",
    "    # print()\n",
    "\n",
    "\n",
    "# Create a DataFrame to store the evaluation metrics for each iteration\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Iteration\": range(1, 6),\n",
    "    \"Accuracy\": accuracy_list,\n",
    "    \"Precision\": precision_list,\n",
    "    \"Recall\": recall_list,\n",
    "    \"F1 Score\": f1_list\n",
    "})\n",
    "\n",
    "# Print the metrics table\n",
    "print(\"Metrics Table:\")\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score_time        0.007607\n",
    "test_accuracy     0.744444\n",
    "test_precision    0.736559\n",
    "test_recall       0.846667\n",
    "test_f1           0.784980\n",
    "test_roc_auc      0.805278"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# nb_results = cross_validate(LogisticRegression(), standardizedX, Y, cv=5, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"neg_log_loss\"])\n",
    "# results_df = pd.DataFrame(nb_results)\n",
    "# print(\"standardizedX\", \"\\n\", results_df.mean(), \"\\n\\n\")\n",
    "\n",
    "\n",
    "# Sistema\n",
    "\n",
    "nb_results = cross_validate(LogisticRegression(), one_hot_standardizedX, Y, cv=5, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"])\n",
    "results_df = pd.DataFrame(nb_results)\n",
    "print(\"one_hot_standardizedX\",\"\\n\", results_df.mean(), \"\\n\\n\")\n",
    "\n",
    "# nb_results = cross_validate(LogisticRegression(), one_hot_normalizedX, Y, cv=5, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"neg_log_loss\"])\n",
    "# results_df = pd.DataFrame(nb_results)\n",
    "# print(\"one_hot_normalizedX\",\"\\n\", results_df.mean(), \"\\n\\n\")\n",
    "\n",
    "param_grid = [    \n",
    "    {\n",
    "    'C' : [0.01,0.1,1,10,100],\n",
    "    }\n",
    "]\n",
    "\n",
    "clf = GridSearchCV(LogisticRegression(), param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"], return_train_score=True, refit=False)\n",
    "clf.fit(one_hot_standardizedX, Y)\n",
    "results_gs = pd.DataFrame(clf.cv_results_)\n",
    "results_gs[['param_C','mean_test_accuracy','mean_test_precision','mean_test_recall','mean_test_f1','mean_test_roc_auc']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = GridSearchCV(KNeighborsClassifier(), {\"n_neighbors\": [1, 3, 5, 7, 9]}, cv=5, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"], return_train_score=True, refit=False)\n",
    "clf.fit(one_hot_standardizedX, Y)\n",
    "results_gs = pd.DataFrame(clf.cv_results_)\n",
    "results_gs[['param_n_neighbors','mean_test_accuracy','mean_test_precision','mean_test_recall','mean_test_f1','mean_test_roc_auc']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>mean_train_accuracy</th>\n",
       "      <th>mean_train_precision</th>\n",
       "      <th>mean_train_recall</th>\n",
       "      <th>mean_train_f1</th>\n",
       "      <th>mean_train_roc_auc</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>mean_test_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.767593</td>\n",
       "      <td>0.797688</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.788497</td>\n",
       "      <td>0.766042</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.746657</td>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.748423</td>\n",
       "      <td>0.718333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.860185</td>\n",
       "      <td>0.844616</td>\n",
       "      <td>0.918333</td>\n",
       "      <td>0.879533</td>\n",
       "      <td>0.912951</td>\n",
       "      <td>0.803704</td>\n",
       "      <td>0.785895</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.835662</td>\n",
       "      <td>0.831250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.945370</td>\n",
       "      <td>0.926742</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.952299</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>0.770370</td>\n",
       "      <td>0.771774</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.800760</td>\n",
       "      <td>0.781389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.980428</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.987599</td>\n",
       "      <td>0.999149</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.739527</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.742472</td>\n",
       "      <td>0.704444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0.999074</td>\n",
       "      <td>0.998347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999170</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.767241</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.766369</td>\n",
       "      <td>0.737500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  param_max_depth  mean_train_accuracy  mean_train_precision  \\\n",
       "0               1             0.767593              0.797688   \n",
       "1               3             0.860185              0.844616   \n",
       "2               5             0.945370              0.926742   \n",
       "3               7             0.986111              0.980428   \n",
       "4               9             0.999074              0.998347   \n",
       "\n",
       "   mean_train_recall  mean_train_f1  mean_train_roc_auc  mean_test_accuracy  \\\n",
       "0           0.780000       0.788497            0.766042            0.722222   \n",
       "1           0.918333       0.879533            0.912951            0.803704   \n",
       "2           0.980000       0.952299            0.983516            0.770370   \n",
       "3           0.995000       0.987599            0.999149            0.711111   \n",
       "4           1.000000       0.999170            0.999991            0.740741   \n",
       "\n",
       "   mean_test_precision  mean_test_recall  mean_test_f1  mean_test_roc_auc  \n",
       "0             0.746657          0.753333      0.748423           0.718333  \n",
       "1             0.785895          0.893333      0.835662           0.831250  \n",
       "2             0.771774          0.833333      0.800760           0.781389  \n",
       "3             0.739527          0.746667      0.742472           0.704444  \n",
       "4             0.767241          0.766667      0.766369           0.737500  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_DT = X[[\"thalach\", \"oldpeak\", \"ca\", \"exang\"]].copy()\n",
    "\n",
    "clf = GridSearchCV(DecisionTreeClassifier(), {\"max_depth\": [1, 3, 5, 7, 9]}, cv=5, scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"], return_train_score=True, refit=False)\n",
    "clf.fit(X, Y)\n",
    "results_gs = pd.DataFrame(clf.cv_results_)\n",
    "results_gs[['param_max_depth','mean_train_accuracy','mean_train_precision','mean_train_recall','mean_train_f1','mean_train_roc_auc', 'mean_test_accuracy','mean_test_precision','mean_test_recall','mean_test_f1','mean_test_roc_auc']]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
